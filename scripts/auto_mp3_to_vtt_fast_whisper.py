"""faster-whisper ã§éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã—ã€VTT ã«ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚

ä½¿ã„æ–¹:
1. audio_input/ ã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmp3/mp4/m4a/wavï¼‰ã‚’é…ç½®
2. AUDIO_FILENAME ã‚’å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›´
3. `pip install -r requirements.txt`ï¼ˆåˆå›ã¯ãƒ¢ãƒ‡ãƒ«è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
4. `python scripts/auto_mp3_to_vtt_fast_whisper.py` ã‚’å®Ÿè¡Œ
5. vtt_output/ ã« .vtt ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‡ºåŠ›ã•ã‚Œã¾ã™
"""

from __future__ import annotations

from pathlib import Path
from typing import Iterable, List, Tuple

from faster_whisper import WhisperModel

# ===================== è¨­å®š =====================
# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
AUDIO_DIR = Path("audio_input")

# VTT ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
VTT_DIR = Path("vtt_output")

# å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ã®å ´åˆã¯è‡ªå‹•æ¢ç´¢ï¼‰
AUDIO_FILENAME = "sample.mp3"

# æ¢ç´¢å¯¾è±¡ã®æ‹¡å¼µå­ï¼ˆå„ªå…ˆé †ï¼‰
ALLOWED_EXTS = [".mp3", ".mp4", ".m4a", ".mov", ".wav"]

# Whisper ãƒ¢ãƒ‡ãƒ«åï¼ˆç²¾åº¦å„ªå…ˆ: "large-v2" / é€Ÿåº¦å„ªå…ˆ: "small"ï¼‰
MODEL_NAME = "large-v2"

# ãƒ‡ãƒã‚¤ã‚¹: "auto" / "cpu" / "cuda" / "metal"ï¼ˆMac ã®å ´åˆã¯ auto ã§ OKï¼‰
DEVICE = "auto"

# è¨ˆç®—ç²¾åº¦: "auto" / "float16" / "int8_float16" ãªã©
COMPUTE_TYPE = "auto"

# å¥èª­ç‚¹ã¨ã—ã¦èªè­˜ã™ã‚‹æ–‡å­—
SENTENCE_PUNCT = "ã€‚.!?ï¼ï¼Ÿ"

# æ–‡ã‚’åˆ†ã‘ã‚‹ç„¡éŸ³ã‚®ãƒ£ãƒƒãƒ—ã®ã—ãã„å€¤ï¼ˆç§’ï¼‰
MAX_GAP_SECONDS = 0.2

# å¥èª­ç‚¹ãŒãªãã¦ã‚‚å¼·åˆ¶åˆ†å‰²ã™ã‚‹é•·ã•ï¼ˆç§’ï¼‰ã€‚None ã§ç„¡åŠ¹
MAX_SENTENCE_SECONDS: float | None = None

# å¥èª­ç‚¹ãŒãªãã¦ã‚‚å¼·åˆ¶åˆ†å‰²ã™ã‚‹å˜èªæ•°ã€‚None ã§ç„¡åŠ¹
MAX_SENTENCE_WORDS: int | None = None
# ===================== è¨­å®šã“ã“ã¾ã§ =====================


def format_timestamp(seconds: float) -> str:
    """ç§’æ•°ã‚’ VTT ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼ã«å¤‰æ›ã™ã‚‹ã€‚"""
    millis = int(round(seconds * 1000))
    hours, remainder = divmod(millis, 3600 * 1000)
    minutes, remainder = divmod(remainder, 60 * 1000)
    secs, ms = divmod(remainder, 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}.{ms:03d}"


def resolve_audio_path(filename: str, search_dir: Path, allowed_exts: List[str]) -> Path:
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è§£æ±ºã™ã‚‹ã€‚æ‹¡å¼µå­ãªã—ã®å ´åˆã¯è‡ªå‹•æ¢ç´¢ã€‚"""
    base_path = search_dir / filename

    if base_path.suffix:
        if base_path.exists():
            return base_path
        stem = base_path.stem
    else:
        stem = filename

    for ext in allowed_exts:
        candidate = search_dir / f"{stem}{ext}"
        if candidate.exists():
            return candidate

    exts_str = ", ".join(allowed_exts)
    raise FileNotFoundError(f"éŸ³å£°/å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {stem} ({exts_str})")


def transcribe_to_vtt(audio_path: Path) -> str:
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—ã—ã¦ VTT å½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
    if not audio_path.exists():
        raise FileNotFoundError(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")

    model = WhisperModel(MODEL_NAME, device=DEVICE, compute_type=COMPUTE_TYPE)
    segments, _info = model.transcribe(
        str(audio_path),
        beam_size=5,
        word_timestamps=True,
        vad_filter=False,
    )

    sentence_segments = split_into_sentences(segments)
    lines: list[str] = ["WEBVTT", ""]
    for start_sec, end_sec, text in sentence_segments:
        lines.append(f"{format_timestamp(start_sec)} --> {format_timestamp(end_sec)}")
        lines.append(text)
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def split_into_sentences(segments: Iterable) -> List[Tuple[float, float, str]]:
    """Whisper ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å¥èª­ç‚¹ãƒ»ç„¡éŸ³ã‚®ãƒ£ãƒƒãƒ—ã§æ–‡å˜ä½ã«åˆ†å‰²ã™ã‚‹ã€‚"""
    sentences: List[Tuple[float, float, str]] = []

    for seg in segments:
        words = seg.words or []
        if not words:
            sentences.append((seg.start, seg.end, seg.text.strip()))
            continue

        buffer: list[str] = []
        start_time = words[0].start
        last_end = words[0].end

        for w in words:
            word_text = w.word
            start_w, end_w = w.start, w.end

            # ç„¡éŸ³ã‚®ãƒ£ãƒƒãƒ—ã§åˆ†å‰²
            if buffer and (start_w - last_end) > MAX_GAP_SECONDS:
                sentences.append((start_time, last_end, "".join(buffer).strip()))
                buffer = []
                start_time = start_w

            buffer.append(word_text)
            last_end = end_w

            punct_hit = word_text.strip() and word_text.strip()[-1] in SENTENCE_PUNCT
            over_time = (
                MAX_SENTENCE_SECONDS is not None
                and start_time is not None
                and (last_end - start_time) >= MAX_SENTENCE_SECONDS
            )
            over_words = MAX_SENTENCE_WORDS is not None and len(buffer) >= MAX_SENTENCE_WORDS

            if punct_hit or over_time or over_words:
                sentences.append((start_time, last_end, "".join(buffer).strip()))
                buffer = []
                start_time = None

        if buffer:
            sentences.append((start_time or words[0].start, last_end, "".join(buffer).strip()))

    return sentences


def save_vtt(vtt_text: str, output_path: Path) -> None:
    """VTT ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(vtt_text, encoding="utf-8")


def main() -> None:
    audio_path = resolve_audio_path(AUDIO_FILENAME, AUDIO_DIR, ALLOWED_EXTS)
    output_path = VTT_DIR / audio_path.with_suffix(".vtt").name

    print(f"ğŸ™ æ–‡å­—èµ·ã“ã—é–‹å§‹: {audio_path}")
    print(f"ğŸ“‚ å‡ºåŠ›å…ˆ: {output_path}")
    vtt_text = transcribe_to_vtt(audio_path)
    save_vtt(vtt_text, output_path)
    print(f"âœ… å®Œäº†: {output_path}")


if __name__ == "__main__":
    main()
